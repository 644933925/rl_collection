{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597141809722",
   "display_name": "Python 3.7.7 64-bit ('tensorflow-gpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FlappyBIrd-DDQN Experiment\n",
    "# 2020/08/11 SYC \n",
    "\n",
    "import models.ddqn as DDQN\n",
    "import models.expStrategy.epsilonGreedy as EPSG\n",
    "import envs.flappyBird as Game\n",
    "import models.util as Util\n",
    "import os\n",
    "import logging\n",
    "# To run tqdm on notebook, import tqdm.notebook\n",
    "# from tqdm.notebook import tqdm\n",
    "# Run on pure python\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config Logging format\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "# Config logging module to enable on notebook\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Block any pop-up windows\n",
    "os.environ['SDL_VIDEODRIVER'] = 'dummy'\n",
    "\n",
    "# Test GPU and show the available logical & physical GPUs\n",
    "Util.test_gpu()\n",
    "\n",
    "game = Game.FlappyBirdEnv()\n",
    "NUM_STATE_FEATURES = game.get_num_state_features()\n",
    "NUM_ACTIONS = game.get_num_actions()\n",
    "BATCH_SIZE = 32\n",
    "EPISODE_NUM = 20\n",
    "PRINT_EVERY_EPISODE = 20\n",
    "\n",
    "exp_stg = EPSG.EpsilonGreedy(0.1, NUM_ACTIONS)\n",
    "agent = DDQN.Agent((NUM_STATE_FEATURES, ), NUM_ACTIONS, 1000, 0.9, 1e-5, exp_stg)\n",
    "\n",
    "state = game.reset()\n",
    "accum_reward = 0\n",
    "bar = []\n",
    "logging.info(\"Episode 1\")\n",
    "for episode in range(1, EPISODE_NUM + 1):\n",
    "    \n",
    "    if episode % PRINT_EVERY_EPISODE == 1:\n",
    "        if episode > 1:\n",
    "            bar.close()\n",
    "            logging.info(\"Accumulated Reward: {} | Loss: {}\".format(round(accum_reward / PRINT_EVERY_EPISODE), agent.get_metrics_loss()))\n",
    "            logging.info(\"Episode {}\".format(episode))\n",
    "            agent.reset_metrics_loss()\n",
    "            accum_reward = 0\n",
    "        bar = tqdm(total = PRINT_EVERY_EPISODE)\n",
    "\n",
    "    while not game.is_over():\n",
    "        action = agent.select_action(state)\n",
    "        state_prime, reward, is_done, info = game.act(action)\n",
    "        # print(f'B State: {state}, Action: {action}, Reward: {reward}, State_Prime: {state_prime}')\n",
    "\n",
    "        agent.add_buffer(state, action, reward, state_prime)\n",
    "        is_update_target = agent.update(BATCH_SIZE)\n",
    "\n",
    "        state = state_prime\n",
    "        accum_reward += reward\n",
    "\n",
    "    bar.update(1)        \n",
    "    game.reset()\n",
    "\n",
    "logging.info(\"Accumulated Reward: {} | Loss: {}\".format(round(accum_reward / PRINT_EVERY_EPISODE), agent.get_metrics_loss()))\n",
    "agent.reset_metrics_loss()\n",
    "bar.close()\n",
    "\n",
    "# Evaluate the model\n",
    "agent.shutdown_explore()\n",
    "agent.reset_metrics_loss()\n",
    "# Reset Game\n",
    "state = game.reset()\n",
    "accum_reward = 0\n",
    "\n",
    "while not game.is_over():\n",
    "    action = agent.select_action(state)\n",
    "    state_prime, reward, is_done, info = game.act(action)\n",
    "\n",
    "    state = state_prime\n",
    "    accum_reward += reward\n",
    "    \n",
    "logging.info(\"Evaluate\")\n",
    "logging.info(\"Accumulated Reward: {}\".format(accum_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1 Physical GPUs, 1 Logical GPUs\n2020-08-11 18:46:21,041 - Episode 1\n100%|██████████| 20/20 [00:00<00:00, 22.85it/s]\n2020-08-11 18:46:21,920 - Accumulated Reward: 13 | Loss: 0.1886632740497589\n2020-08-11 18:46:21,935 - Evaluate\n2020-08-11 18:46:21,937 - Accumulated Reward: 12.0\n"
    }
   ],
   "source": [
    "# CartPole-REINFORCE Experiment\n",
    "# 2020/08/11 SYC \n",
    "\n",
    "import models.REINFORCE as REINFORCE\n",
    "import models.expStrategy.epsilonGreedy as EPSG\n",
    "import envs.cartPole as cartPole\n",
    "import models.util as Util\n",
    "import logging\n",
    "import matplotlib as plt\n",
    "# To run tqdm on notebook, import tqdm.notebook\n",
    "# from tqdm.notebook import tqdm\n",
    "# Run on pure python\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config Logging format\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "# Config logging module to enable on notebook\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Test GPU and show the available logical & physical GPUs\n",
    "Util.test_gpu()\n",
    "\n",
    "env = cartPole.CartPoleEnv()\n",
    "NUM_STATE_FEATURES = env.get_num_state_features()\n",
    "NUM_ACTIONS = env.get_num_actions()\n",
    "BATCH_SIZE = 32\n",
    "EPISODE_NUM = 20\n",
    "PRINT_EVERY_EPISODE = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "REWARD_DISCOUNT = 0.9\n",
    "\n",
    "exp_stg = EPSG.EpsilonGreedy(0.1, NUM_ACTIONS)\n",
    "agent = REINFORCE.Agent((NUM_STATE_FEATURES, ), NUM_ACTIONS, REWARD_DISCOUNT, LEARNING_RATE, exp_stg)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "accum_reward = 0\n",
    "bar = []\n",
    "logging.info(\"Episode 1\")\n",
    "for episode in range(1, EPISODE_NUM + 1):\n",
    "    \n",
    "    if episode % PRINT_EVERY_EPISODE == 1:\n",
    "        if episode > 1:\n",
    "            bar.close()\n",
    "            logging.info(\"Accumulated Reward: {} | Loss: {}\".format(round(accum_reward / PRINT_EVERY_EPISODE), agent.get_metrics_loss()))\n",
    "            logging.info(\"Episode {}\".format(episode))\n",
    "            agent.reset_metrics_loss()\n",
    "            accum_reward = 0\n",
    "        bar = tqdm(total = PRINT_EVERY_EPISODE)\n",
    "\n",
    "    while not env.is_over():\n",
    "        # env.render()\n",
    "        action = agent.select_action(state)\n",
    "        state_prime, reward, is_done, info = env.act(action)\n",
    "\n",
    "        agent.add_buffer(state, action, reward, state_prime)\n",
    "        # print(f'State: {state}, Action: {action}, Reward: {reward}, State_Prime: {state_prime}')\n",
    "\n",
    "        state = state_prime\n",
    "        accum_reward += reward\n",
    "\n",
    "    agent.update()\n",
    "    agent.reset_buffer()\n",
    "\n",
    "    bar.update(1)        \n",
    "    env.reset()\n",
    "\n",
    "bar.close()    \n",
    "logging.info(\"Accumulated Reward: {} | Loss: {}\".format(round(accum_reward / PRINT_EVERY_EPISODE), agent.get_metrics_loss()))\n",
    "agent.reset_metrics_loss()\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "agent.shutdown_explore()\n",
    "agent.reset_metrics_loss()\n",
    "# Reset Game\n",
    "env_state = env.reset()\n",
    "accum_reward = 0\n",
    "\n",
    "while not env.is_over():\n",
    "    # env.render()\n",
    "    action = agent.select_action(state)\n",
    "    state_prime, reward, is_done, info = env.act(action)\n",
    "\n",
    "    state = state_prime\n",
    "    accum_reward += reward\n",
    "\n",
    "logging.info(\"Evaluate\")\n",
    "logging.info(\"Accumulated Reward: {}\".format(accum_reward))"
   ]
  }
 ]
}